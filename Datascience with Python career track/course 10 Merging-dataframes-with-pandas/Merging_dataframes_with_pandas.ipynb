{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Merging-dataframes-with-pandas.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "bCdQrIJu1-fS",
        "UyzL8geL2v3R",
        "-GrJOJeWpKOm",
        "OFyA8EQxq91w",
        "EsUHyG9EsH_m",
        "4vraEneBtSVS",
        "ktXB1wSMuEe0",
        "kG8qAvrZxjHG",
        "ZBnfFuONyV1-",
        "1hRH4iBb2grF",
        "LbgnlmfEwiyT",
        "FWh6YpmJ7kW_",
        "sNICOFxRaySE"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCdQrIJu1-fS",
        "colab_type": "text"
      },
      "source": [
        "# Preparing data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UyzL8geL2v3R",
        "colab_type": "text"
      },
      "source": [
        "## Reading multiple data files\n",
        "\n",
        "- pd.read_csv()\n",
        "- pd.read_excel()\n",
        "- pd.read_html()\n",
        "- pd.read_json()\n",
        "\n",
        "***How to read multiple dataframes?***\n",
        "- `dfs = [pd.read_csv(f) for f in filenames]`\n",
        "- when many filenames have similar patterns, we can use glob module\n",
        "  - from glob import glob\n",
        "  - filename = glob('sales*.csv')\n",
        "  - the * is a wildcard, that matches >=0 standard characters\n",
        "  - this creates an iterable list of filenames\n",
        "  - then we can do this `[pd.read_csv(f) for f in filenames]`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GrJOJeWpKOm",
        "colab_type": "text"
      },
      "source": [
        "## Reindexing DataFrames\n",
        "plural - indices/indexes both are correct!\n",
        "\n",
        "Lets adopt the following convention:\n",
        "- indices: many index labels within Index data structures\n",
        "- indexes: multiple index associated with many pandas Index data structures\n",
        "\n",
        "- df.reindex(listoforderedIndices)\n",
        "- df.sort_index() if you want to sort\n",
        "- df.reindex(df2.index)\n",
        "- might want to df.dropna() because reindexing might add nan rows for the indices it didnt have earlier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFyA8EQxq91w",
        "colab_type": "text"
      },
      "source": [
        "### Sorting DataFrame with the Index & columns\n",
        "It is often useful to rearrange the sequence of the rows of a DataFrame by sorting. You don't have to implement these yourself; the principal methods for doing this are .sort_index() and .sort_values().\n",
        "\n",
        "- Read 'monthly_max_temp.csv' into a DataFrame called weather1 with 'Month' as the index.\n",
        "- Sort the index of weather1 in alphabetical order using the .sort_index() method and store the result in weather2.\n",
        "- Sort the index of weather1 in reverse alphabetical order by specifying the additional keyword argument ascending=False inside .sort_index().\n",
        "- Use the .sort_values() method to sort weather1 in increasing numerical order according to the values of the column 'Max TemperatureF'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POryMhaPrnLY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import pandas\n",
        "import pandas as pd\n",
        "\n",
        "# Read 'monthly_max_temp.csv' into a DataFrame: weather1\n",
        "weather1 = pd.read_csv('monthly_max_temp.csv',index_col='Month')\n",
        "\n",
        "# Print the head of weather1\n",
        "print(weather1.head())\n",
        "\n",
        "# Sort the index of weather1 in alphabetical order: weather2\n",
        "weather2 = weather1.sort_index()\n",
        "\n",
        "# Print the head of weather2\n",
        "print(weather2.head())\n",
        "\n",
        "# Sort the index of weather1 in reverse alphabetical order: weather3\n",
        "weather3 = weather1.sort_index(ascending=False)\n",
        "\n",
        "# Print the head of weather3\n",
        "print(weather3.head())\n",
        "\n",
        "# Sort weather1 numerically using the values of 'Max TemperatureF': weather4\n",
        "weather4 = weather1.sort_values('Max TemperatureF')\n",
        "\n",
        "# Print the head of weather4\n",
        "print(weather4.head())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EsUHyG9EsH_m",
        "colab_type": "text"
      },
      "source": [
        "### Reindexing DataFrame from a list\n",
        "Sorting methods are not the only way to change DataFrame Indexes. There is also the .reindex() method.\n",
        "\n",
        "In this exercise, you'll reindex a DataFrame of quarterly-sampled mean temperature values to contain monthly samples (this is an example of upsampling or increasing the rate of samples, which you may recall from the pandas Foundations course).\n",
        "\n",
        "The original data has the first month's abbreviation of the quarter (three-month interval) on the Index, namely Apr, Jan, Jul, and Oct. This data has been loaded into a DataFrame called weather1 and has been printed in its entirety in the IPython Shell. Notice it has only four rows (corresponding to the first month of each quarter) and that the rows are not sorted chronologically.\n",
        "\n",
        "```\n",
        "       Mean TemperatureF\n",
        "Month                   \n",
        "Apr            61.956044\n",
        "Jan            32.133333\n",
        "Jul            68.934783\n",
        "Oct            43.434783\n",
        "```\n",
        "\n",
        "You'll initially use a list of all twelve month abbreviations and subsequently apply the .ffill() method to forward-fill the null entries when upsampling. This list of month abbreviations has been pre-loaded as year.\n",
        "\n",
        "```\n",
        "['Jan',\n",
        " 'Feb',\n",
        " 'Mar',\n",
        " 'Apr',\n",
        " 'May',\n",
        " 'Jun',\n",
        " 'Jul',\n",
        " 'Aug',\n",
        " 'Sep',\n",
        " 'Oct',\n",
        " 'Nov',\n",
        " 'Dec']\n",
        "```\n",
        "\n",
        "- Reorder the rows of weather1 using the .reindex() method with the list year as the argument, which contains the abbreviations for each month.\n",
        "\n",
        "- Reorder the rows of weather1 just as you did above, this time chaining the .ffill() method to replace the null values with the last preceding non-null value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nT65DJHOsVth",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import pandas\n",
        "import pandas as pd\n",
        "\n",
        "# Reindex weather1 using the list year: weather2\n",
        "weather2 = weather1.reindex(year)\n",
        "\n",
        "# Print weather2\n",
        "print(weather2)\n",
        "\n",
        "# Reindex weather1 using the list year with forward-fill: weather3\n",
        "weather3 = weather1.reindex(year).ffill()\n",
        "\n",
        "# Print weather3\n",
        "print(weather3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgdF7b2gs7Eo",
        "colab_type": "text"
      },
      "source": [
        "Output:\n",
        "```\n",
        "           Mean TemperatureF\n",
        "    Month                   \n",
        "    Jan            32.133333\n",
        "    Feb                  NaN\n",
        "    Mar                  NaN\n",
        "    Apr            61.956044\n",
        "    May                  NaN\n",
        "    Jun                  NaN\n",
        "    Jul            68.934783\n",
        "    Aug                  NaN\n",
        "    Sep                  NaN\n",
        "    Oct            43.434783\n",
        "    Nov                  NaN\n",
        "    Dec                  NaN\n",
        "           Mean TemperatureF\n",
        "    Month                   \n",
        "    Jan            32.133333\n",
        "    Feb            32.133333\n",
        "    Mar            32.133333\n",
        "    Apr            61.956044\n",
        "    May            61.956044\n",
        "    Jun            61.956044\n",
        "    Jul            68.934783\n",
        "    Aug            68.934783\n",
        "    Sep            68.934783\n",
        "    Oct            43.434783\n",
        "    Nov            43.434783\n",
        "    Dec            43.434783\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vraEneBtSVS",
        "colab_type": "text"
      },
      "source": [
        "### Reindexing using another DataFrame Index\n",
        "Another common technique is to reindex a DataFrame using the Index of another DataFrame. The DataFrame .reindex() method can accept the Index of a DataFrame or Series as input. You can access the Index of a DataFrame with its .index attribute.\n",
        "\n",
        "The Baby Names Dataset from data.gov summarizes counts of names (with genders) from births registered in the US since 1881. In this exercise, you will start with two baby-names DataFrames names_1981 and names_1881 loaded for you.\n",
        "\n",
        "The DataFrames names_1981 and names_1881 both have a MultiIndex with levels name and gender giving unique labels to counts in each row. If you're interested in seeing how the MultiIndexes were set up, names_1981 and names_1881 were read in using the following commands:\n",
        "```\n",
        "names_1981 = pd.read_csv('names1981.csv', header=None, names=['name','gender','count'], index_col=(0,1))\n",
        "names_1881 = pd.read_csv('names1881.csv', header=None, names=['name','gender','count'], index_col=(0,1))\n",
        "```\n",
        "As you can see by looking at their shapes, which have been printed in the IPython Shell, the DataFrame corresponding to 1981 births is much larger, reflecting the greater diversity of names in 1981 as compared to 1881.\n",
        "```\n",
        "Shape of names_1981 DataFrame: (19455, 1)\n",
        "Shape of names_1881 DataFrame: (1935, 1)\n",
        "\n",
        "```\n",
        "\n",
        "Your job here is to use the DataFrame .reindex() and .dropna() methods to make a DataFrame common_names counting names from 1881 that were still popular in 1981.\n",
        "\n",
        "- Create a new DataFrame common_names by reindexing names_1981 using the index attribute of the DataFrame names_1881 of older names.\n",
        "- Print the shape of the new common_names DataFrame. This has been done for you. It should be the same as that of names_1881.\n",
        "- Drop the rows of common_names that have null counts using the .dropna() method. These rows correspond to names that fell out of fashion between 1881 & 1981.\n",
        "- Print the shape of the reassigned common_names DataFrame. This has been done for you, so hit 'Submit Answer' to see the result!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPDyiXwts616",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import pandas\n",
        "import pandas as pd\n",
        "\n",
        "# Reindex names_1981 with index of names_1881: common_names\n",
        "common_names = names_1981.reindex(names_1881.index)\n",
        "\n",
        "# Print shape of common_names\n",
        "print(common_names.shape)\n",
        "\n",
        "# Drop rows with null counts: common_names\n",
        "common_names = common_names.dropna()\n",
        "\n",
        "# Print shape of new common_names\n",
        "print(common_names.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktXB1wSMuEe0",
        "colab_type": "text"
      },
      "source": [
        "## Arithmetic with Series and DataFrames\n",
        "\n",
        "- broadcasting possible, of a scalar\n",
        "- dividing a dataframe(each column) with a series: df1.divide(series1, axis='rows')\n",
        "- percenatge change along the time series: df.pct_change()*100 = (curr-prev)/prev\n",
        "- df1 + df2 for similar column named dfs\n",
        "- same thing by df1.add(df2)\n",
        "- df1.add(df2, fill_value=0)\n",
        "- triple sum : df1.add(df2,fill_value=0).add(df3, fill_value=0)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlIrQEW0v0Gu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Converting temperatue units and renaming the column\n",
        "'''\n",
        "# Extract selected columns from weather as new DataFrame: temps_f\n",
        "temps_f = weather[['Min TemperatureF', 'Mean TemperatureF', 'Max TemperatureF']]\n",
        "\n",
        "# Convert temps_f to celsius: temps_c\n",
        "temps_c = (temps_f-32)*5/9\n",
        "\n",
        "# Rename 'F' in column names with 'C': temps_c.columns\n",
        "temps_c.columns = temps_c.columns.str.replace('F','C')\n",
        "\n",
        "# Print first 5 rows of temps_c\n",
        "print(temps_c.head())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kG8qAvrZxjHG",
        "colab_type": "text"
      },
      "source": [
        "### Computing percentage growth of GDP\n",
        "Your job in this exercise is to compute the yearly percent-change of US GDP (Gross Domestic Product) since 2008.\n",
        "\n",
        "The data has been obtained from the Federal Reserve Bank of St. Louis and is available in the file GDP.csv, which contains quarterly data; you will resample it to annual sampling and then compute the annual growth of GDP. For a refresher on resampling, check out the relevant material from pandas Foundations.\n",
        "\n",
        "- Read the file 'GDP.csv' into a DataFrame called gdp, using parse_dates=True and index_col='DATE'.\n",
        "- Create a DataFrame post2008 by slicing gdp such that it comprises all rows from 2008 onward.\n",
        "- Print the last 8 rows of the slice post2008. This has been done for you. This data has quarterly frequency so the indices are separated by three-month intervals.\n",
        "- Create the DataFrame yearly by resampling the slice post2008 by year. Remember, you need to chain .resample() (using the alias 'A' for annual frequency) with some kind of aggregation; you will use the aggregation method .last() to select the last element when resampling.\n",
        "- Compute the percentage growth of the resampled DataFrame yearly with .pct_change() * 100."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JErApZ9mv0Dg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read 'GDP.csv' into a DataFrame: gdp\n",
        "gdp = pd.read_csv('GDP.csv',parse_dates=True , index_col='DATE')\n",
        "\n",
        "# Slice all the gdp data from 2008 onward: post2008\n",
        "post2008 = gdp.loc['2008':]\n",
        "\n",
        "# Print the last 8 rows of post2008\n",
        "print(post2008.tail(8))\n",
        "\n",
        "# Resample post2008 by year, keeping last(): yearly\n",
        "yearly = post2008.resample('A').last()\n",
        "\n",
        "# Print yearly\n",
        "print(yearly)\n",
        "\n",
        "# Compute percentage growth of yearly: yearly['growth']\n",
        "yearly['growth'] = yearly.pct_change() * 100\n",
        "\n",
        "# Print yearly again\n",
        "print(yearly)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBnfFuONyV1-",
        "colab_type": "text"
      },
      "source": [
        "### Converting currency of stocks\n",
        "In this exercise, stock prices in US Dollars for the S&P 500 in 2015 have been obtained from Yahoo Finance. The files sp500.csv for sp500 and exchange.csv for the exchange rates are both provided to you.\n",
        "\n",
        "Using the daily exchange rate to Pounds Sterling, your task is to convert both the Open and Close column prices.\n",
        "\n",
        "- Read the DataFrames sp500 & exchange from the files 'sp500.csv' & 'exchange.csv' respectively..\n",
        "- Use parse_dates=True and index_col='Date'.\n",
        "- Extract the columns 'Open' & 'Close' from the DataFrame sp500 as a new DataFrame dollars and print the first 5 rows.\n",
        "- Construct a new DataFrame pounds by converting US dollars to British pounds. You'll use the .multiply() method of dollars with exchange['GBP/USD'] and axis='rows'\n",
        "- Print the first 5 rows of the new DataFrame pounds. This has been done for you, so hit 'Submit Answer' to see the results!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yItJZx6NyehW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import pandas\n",
        "import pandas as pd\n",
        "\n",
        "# Read 'sp500.csv' into a DataFrame: sp500\n",
        "sp500 = pd.read_csv('sp500.csv',parse_dates=True, index_col='Date')\n",
        "\n",
        "# Read 'exchange.csv' into a DataFrame: exchange\n",
        "exchange = pd.read_csv('exchange.csv',parse_dates=True, index_col='Date')\n",
        "\n",
        "# Subset 'Open' & 'Close' columns from sp500: dollars\n",
        "dollars = sp500[['Open','Close']]\n",
        "\n",
        "# Print the head of dollars\n",
        "print(dollars.head())\n",
        "\n",
        "# Convert dollars to pounds: pounds\n",
        "pounds = dollars.multiply(exchange['GBP/USD'],axis='rows')\n",
        "\n",
        "# Print the head of pounds\n",
        "print(pounds.head())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hRH4iBb2grF",
        "colab_type": "text"
      },
      "source": [
        "# Concatenating data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbgnlmfEwiyT",
        "colab_type": "text"
      },
      "source": [
        "## Appending and concatinating Series\n",
        "\n",
        "**Stacking on top of one another**\n",
        "- s1.append(s2) -- only row-wise -- doesnt change indexes\n",
        "- we can reset index by: s1.append(s2).reset_index(drop=True)\n",
        "- append multiple dataframes/series by chaining: s1.append(s2).append(s3)\n",
        "- pd.concat([s1,s2,s3])  -- more flexible, can concat vertically/horizontally\n",
        "- pd.concat([s1,s2,s3], ignore_index=True)\n",
        "\n",
        "## Appending and concatinating DataFrames\n",
        "\n",
        "**concat**\n",
        "- if they have the same index and column names, they will stack uo as expected\n",
        "- with different indexes and column, if will form all the different columns and rows with NaNs, just a **UNION** occurs.\n",
        "-even the same named rows are repeated\n",
        "- by default concat is axis=0, i.e stacking rows-wise\n",
        "- with axis=1 or axis='columns', the same named indexes get alligned, columns are concatinated. NaNs are put in relevant places."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAMn0TtU55Co",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Add 'year' column to names_1881 and names_1981\n",
        "names_1881['year'] = 1881\n",
        "names_1981['year'] = 1981\n",
        "\n",
        "# Append names_1981 after names_1881 with ignore_index=True: combined_names\n",
        "combined_names = pd.concat([names_1881,names_1981],ignore_index=True)\n",
        "\n",
        "# Print shapes of names_1981, names_1881, and combined_names\n",
        "print(names_1981.shape)\n",
        "print(names_1881.shape)\n",
        "print(combined_names.shape)\n",
        "\n",
        "# Print all rows that contain the name 'Morgan'\n",
        "print(combined_names[combined_names['name']=='Morgan'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n06Nft9_56Lc",
        "colab_type": "text"
      },
      "source": [
        "Output \n",
        "\n",
        "```\n",
        "    (19455, 4)\n",
        "    (1935, 4)\n",
        "    (21390, 4)\n",
        "             name gender  count  year\n",
        "    1283   Morgan      M     23  1881\n",
        "    2096   Morgan      F   1769  1981\n",
        "    14390  Morgan      M    766  1981\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOoCRG1X7gJA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Initialize an empyy list: medals\n",
        "medals =[]\n",
        "\n",
        "for medal in medal_types:\n",
        "    # Create the file name: file_name\n",
        "    file_name = \"%s_top5.csv\" % medal\n",
        "    # Create list of column names: columns\n",
        "    columns = ['Country', medal]\n",
        "    # Read file_name into a DataFrame: medal_df\n",
        "    medal_df = pd.read_csv(file_name,header=0,index_col='Country',names=columns)\n",
        "    # Append medal_df to medals\n",
        "    medals.append(medal_df)\n",
        "\n",
        "# Concatenate medals horizontally: medals_df\n",
        "medals_df = pd.concat(medals,axis='columns')\n",
        "\n",
        "# Print medals_df\n",
        "print(medals_df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWh6YpmJ7kW_",
        "colab_type": "text"
      },
      "source": [
        "## Concatenation, keys, & MultiIndexes\n",
        "\n",
        "- two different dfs, same index names, same column name, if concat, will just stack verticaly. If we want to differentiate them, one thing we could do is:\n",
        "  - `pd.concat([df1,df2], keys=[2013,2014])`\n",
        "  - this creates **multi-index**, with each df getting an outer index of 2013 and 2014 respectively.\n",
        "- another way is:\n",
        "  - to concat `axis=1`\n",
        "  - unfortunately it may give two same named columns, we can create **multi-level column** by `pd.concat([df1,df2], keys=[2013,2014], axis=1)`\n",
        "- we can do all these using dict also:\n",
        "  - dd = { 2013: df1, 2014:df2 }\n",
        "  - df = pd.concat(dd, axis=0/1)\n",
        "  - the dict keys are treated as the `keys` argument values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVh0Mskh-4bw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for medal in medal_types:\n",
        "\n",
        "    file_name = \"%s_top5.csv\" % medal\n",
        "    \n",
        "    # Read file_name into a DataFrame: medal_df\n",
        "    medal_df = pd.read_csv(file_name,index_col='Country')\n",
        "    \n",
        "    # Append medal_df to medals\n",
        "    medals.append(medal_df)\n",
        "    \n",
        "# Concatenate medals: medals\n",
        "medals = pd.concat(medals,keys=['bronze', 'silver', 'gold'])\n",
        "\n",
        "# Print medals in entirety\n",
        "print(medals)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sH75r95l-z7H",
        "colab_type": "text"
      },
      "source": [
        "output:\n",
        "```\n",
        "                            Total\n",
        "           Country               \n",
        "    bronze United States   1052.0\n",
        "           Soviet Union     584.0\n",
        "           United Kingdom   505.0\n",
        "           France           475.0\n",
        "           Germany          454.0\n",
        "    silver United States   1195.0\n",
        "           Soviet Union     627.0\n",
        "           United Kingdom   591.0\n",
        "           France           461.0\n",
        "           Italy            394.0\n",
        "    gold   United States   2088.0\n",
        "           Soviet Union     838.0\n",
        "           United Kingdom   498.0\n",
        "           Italy            460.0\n",
        "           Germany          407.0\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TF-5kGom-6kT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Sort the entries of medals: medals_sorted\n",
        "medals_sorted = medals.sort_index(level=0)\n",
        "\n",
        "# Print the number of Bronze medals won by Germany\n",
        "print(medals_sorted.loc[('bronze','Germany')])\n",
        "\n",
        "# Print data about silver medals\n",
        "print(medals_sorted.loc['silver'])\n",
        "\n",
        "# Create alias for pd.IndexSlice: idx\n",
        "idx = pd.IndexSlice\n",
        "\n",
        "# Print all the data on medals won by the United Kingdom\n",
        "print(medals_sorted.loc[idx[:,'United Kingdom'], :])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAlPWUf2_kLI",
        "colab_type": "text"
      },
      "source": [
        "OUTPUT:\n",
        "```\n",
        "    Total    454.0\n",
        "    Name: (bronze, Germany), dtype: float64\n",
        "                     Total\n",
        "    Country               \n",
        "    France           461.0\n",
        "    Italy            394.0\n",
        "    Soviet Union     627.0\n",
        "    United Kingdom   591.0\n",
        "    United States   1195.0\n",
        "                           Total\n",
        "           Country              \n",
        "    bronze United Kingdom  505.0\n",
        "    gold   United Kingdom  498.0\n",
        "    silver United Kingdom  591.0\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJzDn0MdYGno",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Concatenate dataframes: february\n",
        "february = pd.concat(dataframes, axis=1, keys=['Hardware', 'Software', 'Service'])\n",
        "\n",
        "# Print february.info()\n",
        "print(february.info())\n",
        "\n",
        "# Assign pd.IndexSlice: idx\n",
        "idx = pd.IndexSlice\n",
        "\n",
        "# Create the slice: slice_2_8\n",
        "slice_2_8 = february.loc['Feb. 2, 2015':'Feb. 8, 2015', idx[:, 'Company']]\n",
        "\n",
        "# Print slice_2_8\n",
        "print(slice_2_8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsNpFX8zYuPi",
        "colab_type": "text"
      },
      "source": [
        "Output:\n",
        "\n",
        "```\n",
        "    <class 'pandas.core.frame.DataFrame'>\n",
        "    DatetimeIndex: 20 entries, 2015-02-02 08:33:01 to 2015-02-26 08:58:51\n",
        "    Data columns (total 9 columns):\n",
        "    (Hardware, Company)    5 non-null object\n",
        "    (Hardware, Product)    5 non-null object\n",
        "    (Hardware, Units)      5 non-null float64\n",
        "    (Software, Company)    9 non-null object\n",
        "    (Software, Product)    9 non-null object\n",
        "    (Software, Units)      9 non-null float64\n",
        "    (Service, Company)     6 non-null object\n",
        "    (Service, Product)     6 non-null object\n",
        "    (Service, Units)       6 non-null float64\n",
        "    dtypes: float64(3), object(6)\n",
        "    memory usage: 1.6+ KB\n",
        "    None\n",
        "                                Hardware         Software Service\n",
        "                                 Company          Company Company\n",
        "    Date                                                         \n",
        "    2015-02-02 08:33:01              NaN            Hooli     NaN\n",
        "    2015-02-02 20:54:49        Mediacore              NaN     NaN\n",
        "    2015-02-03 14:14:18              NaN          Initech     NaN\n",
        "    2015-02-04 15:36:29              NaN        Streeplex     NaN\n",
        "    2015-02-04 21:52:45  Acme Coporation              NaN     NaN\n",
        "    2015-02-05 01:53:06              NaN  Acme Coporation     NaN\n",
        "    2015-02-05 22:05:03              NaN              NaN   Hooli\n",
        "    2015-02-07 22:58:10  Acme Coporation              NaN     NaN\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kfR5zxkZ4_t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Make the list of tuples: month_list\n",
        "month_list = [('january', jan), ('february', feb), ('march', mar)]\n",
        "print(month_list)\n",
        "# Create an empty dictionary: month_dict\n",
        "month_dict = dict()\n",
        "\n",
        "for month_name, month_data in month_list:\n",
        "\n",
        "    # Group month_data: month_dict[month_name]\n",
        "    month_dict[month_name] = month_data.groupby('Company').sum()\n",
        "\n",
        "# Concatenate data in month_dict: sales\n",
        "sales = pd.concat(month_dict)\n",
        "\n",
        "# Print sales\n",
        "print(sales)\n",
        "\n",
        "# Print all sales by Mediacore\n",
        "idx = pd.IndexSlice\n",
        "print(sales.loc[idx[:, 'Mediacore'], :])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pe_Kc5A2Z4xo",
        "colab_type": "text"
      },
      "source": [
        "OUTPUT:\n",
        "\n",
        "```\n",
        "    [('january',\n",
        "                        Date          Company   Product  Units\n",
        "    0   2015-01-21 19:13:21        Streeplex  Hardware     11\n",
        "    1   2015-01-09 05:23:51        Streeplex   Service      8\n",
        "    2   2015-01-06 17:19:34          Initech  Hardware     17\n",
        "    3   2015-01-02 09:51:06            Hooli  Hardware     16\n",
        "    4   2015-01-11 14:51:02            Hooli  Hardware     11\n",
        "    5   2015-01-01 07:31:20  Acme Coporation  Software     18\n",
        "    6   2015-01-24 08:01:16          Initech  Software      1\n",
        "    7   2015-01-25 15:40:07          Initech   Service      6\n",
        "    8   2015-01-13 05:36:12            Hooli   Service      7\n",
        "    9   2015-01-03 18:00:19            Hooli   Service     19\n",
        "    10  2015-01-16 00:33:47            Hooli  Hardware     17\n",
        "    11  2015-01-16 07:21:12          Initech   Service     13\n",
        "    12  2015-01-20 19:49:24  Acme Coporation  Hardware     12\n",
        "    13  2015-01-26 01:50:25  Acme Coporation  Software     14\n",
        "    14  2015-01-15 02:38:25  Acme Coporation   Service     16\n",
        "    15  2015-01-06 13:47:37  Acme Coporation  Software     16\n",
        "    16  2015-01-15 15:33:40        Mediacore  Hardware      7\n",
        "    17  2015-01-27 07:11:55        Streeplex   Service     18\n",
        "    18  2015-01-20 11:28:02        Streeplex  Software     13\n",
        "    19  2015-01-16 19:20:46        Mediacore   Service      8), \n",
        "    ('february',\n",
        "                        Date          Company   Product  Units\n",
        "    0   2015-02-26 08:57:45        Streeplex   Service      4\n",
        "    1   2015-02-16 12:09:19            Hooli  Software     10\n",
        "    2   2015-02-03 14:14:18          Initech  Software     13\n",
        "    3   2015-02-02 08:33:01            Hooli  Software      3\n",
        "    4   2015-02-25 00:29:00          Initech   Service     10\n",
        "    5   2015-02-05 01:53:06  Acme Coporation  Software     19\n",
        "    6   2015-02-09 08:57:30        Streeplex   Service     19\n",
        "    7   2015-02-11 20:03:08          Initech  Software      7\n",
        "    8   2015-02-04 21:52:45  Acme Coporation  Hardware     14\n",
        "    9   2015-02-09 13:09:55        Mediacore  Software      7\n",
        "    10  2015-02-07 22:58:10  Acme Coporation  Hardware      1\n",
        "    11  2015-02-11 22:50:44            Hooli  Software      4\n",
        "    12  2015-02-26 08:58:51        Streeplex   Service      1\n",
        "    13  2015-02-05 22:05:03            Hooli   Service     10\n",
        "    14  2015-02-04 15:36:29        Streeplex  Software     13\n",
        "    15  2015-02-19 16:02:58        Mediacore   Service     10\n",
        "    16  2015-02-19 10:59:33        Mediacore  Hardware     16\n",
        "    17  2015-02-02 20:54:49        Mediacore  Hardware      9\n",
        "    18  2015-02-21 05:01:26        Mediacore  Software      3\n",
        "    19  2015-02-21 20:41:47            Hooli  Hardware      3),\n",
        "     ('march',\n",
        "                         Date          Company   Product  Units\n",
        "    0   2015-03-22 14:42:25        Mediacore  Software      6\n",
        "    1   2015-03-12 18:33:06          Initech   Service     19\n",
        "    2   2015-03-22 03:58:28        Streeplex  Software      8\n",
        "    3   2015-03-15 00:53:12            Hooli  Hardware     19\n",
        "    4   2015-03-17 19:25:37            Hooli  Hardware     10\n",
        "    5   2015-03-16 05:54:06        Mediacore  Software      3\n",
        "    6   2015-03-25 10:18:10          Initech  Hardware      9\n",
        "    7   2015-03-25 16:42:42        Streeplex  Hardware     12\n",
        "    8   2015-03-26 05:20:04        Streeplex  Software      3\n",
        "    9   2015-03-06 10:11:45        Mediacore  Software     17\n",
        "    10  2015-03-22 21:14:39          Initech  Hardware     11\n",
        "    11  2015-03-17 19:38:12            Hooli  Hardware      8\n",
        "    12  2015-03-28 19:20:38  Acme Coporation   Service      5\n",
        "    13  2015-03-13 04:41:32        Streeplex  Hardware      8\n",
        "    14  2015-03-06 02:03:56        Mediacore  Software     17\n",
        "    15  2015-03-13 11:40:16          Initech  Software     11\n",
        "    16  2015-03-27 08:29:45        Mediacore  Software      6\n",
        "    17  2015-03-21 06:42:41        Mediacore  Hardware     19\n",
        "    18  2015-03-15 08:50:45          Initech  Hardware     18\n",
        "    19  2015-03-13 16:25:24        Streeplex  Software      9)]\n",
        "                              \n",
        "                              Units\n",
        "             Company               \n",
        "    february Acme Coporation     34\n",
        "             Hooli               30\n",
        "             Initech             30\n",
        "             Mediacore           45\n",
        "             Streeplex           37\n",
        "    january  Acme Coporation     76\n",
        "             Hooli               70\n",
        "             Initech             37\n",
        "             Mediacore           15\n",
        "             Streeplex           50\n",
        "    march    Acme Coporation      5\n",
        "             Hooli               37\n",
        "             Initech             68\n",
        "             Mediacore           68\n",
        "             Streeplex           40\n",
        "                        Units\n",
        "             Company         \n",
        "    february Mediacore     45\n",
        "    january  Mediacore     15\n",
        "    march    Mediacore     68\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNICOFxRaySE",
        "colab_type": "text"
      },
      "source": [
        "## In numpy arrays:\n",
        "\n",
        "- **horizontal** stacking: same # of row, # of columns can differ\n",
        "- np.hstack([A,B])\n",
        "- np.concatenate([A,B], axis=1)\n",
        "- **vertical** stacking: same # of col, # of rows can differ\n",
        "- np.vstack([A,B])\n",
        "- np.concatenate([A,B], axis=0)\n",
        "\n",
        "## Joins:\n",
        "\n",
        "- **Outer join** :\n",
        "  - union of index sets (all labels, no reps)\n",
        "  - missing values filled with NaN\n",
        "  - pd.concat([df1,df2],axis=1,join='outer') #default is 'outer'\n",
        "- **Inner join** :\n",
        "  - intersection of index sets (only common labels)\n",
        "  - pd.concat([df1,df2],axis=1,join='inner')\n",
        "- can also do inner and outer for axis=0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RpNo2zOwaxsL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CKo3YSo2gnd",
        "colab_type": "text"
      },
      "source": [
        "# Merging data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIC8VkW7nI-f",
        "colab_type": "text"
      },
      "source": [
        "## Mergining DataFrames\n",
        "\n",
        "- merge extends concat, with the ability to **align rows using multiple columns**\n",
        "- `pd.merge(df1,df2)` : merges based on all columns that occur in *both* dataframes, values have to be silimar too, this is by-deafult, an inner join\n",
        "- `pd.merge(a,b,on='col')`: merges based on 'col' column.\n",
        "- `pd.merge(a,b,on=['col1','col2'],suffixes=['_year1','year2])` to change the default suffix of x and y.\n",
        "- what if the column names differ?(but are the same, so we want to merge on it): `pd.merge(a,b,left_on='col1',right_on='col2')`\n",
        "\n",
        "## Joining DataFrames:\n",
        "\n",
        "- `pd.merge(a,b,left_on='col1',right_on='col2',how='inner')` #default is inner\n",
        "- how = 'left'\n",
        "- how = 'right'\n",
        "- how = 'outer'\n",
        "\n",
        "- we can also: df1.join(df2, how='left') #how is left by default, and also on the index\n",
        "\n",
        "![What to use?](df.png)\n",
        "\n",
        "## Ordered Merges:\n",
        "\n",
        "- for the indexes which have ordering naturally, e.g date-time\n",
        "- merge+outer+.sorted('Dates') = merge_ordered(df1,df2)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3P_aP2XBnMyh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Perform the first ordered merge: tx_weather\n",
        "tx_weather = pd.merge_ordered(austin,houston)\n",
        "\n",
        "# Print tx_weather\n",
        "print(tx_weather)\n",
        "\n",
        "# Perform the second ordered merge: tx_weather_suff\n",
        "tx_weather_suff = pd.merge_ordered(austin,houston,on='date',suffixes=['_aus','_hus'])\n",
        "\n",
        "# Print tx_weather_suff\n",
        "print(tx_weather_suff)\n",
        "\n",
        "# Perform the third ordered merge: tx_weather_ffill\n",
        "tx_weather_ffill = pd.merge_ordered(austin,houston,on='date',suffixes=['_aus','_hus'],fill_method='ffill')\n",
        "\n",
        "# Print tx_weather_ffill\n",
        "print(tx_weather_ffill)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8s8t8WW1Wxe",
        "colab_type": "text"
      },
      "source": [
        "### Using merge_asof()\n",
        "Similar to pd.merge_ordered(), the pd.merge_asof() function will also merge values in order using the on column, but for each row in the left DataFrame, only rows from the right DataFrame whose 'on' column values are less than the left value will be kept.\n",
        "\n",
        "This function can be used to align disparate datetime frequencies without having to first resample.\n",
        "\n",
        "Here, you'll merge monthly oil prices (US dollars) into a full automobile fuel efficiency dataset. The oil and automobile DataFrames have been pre-loaded as oil and auto. The first 5 rows of each have been printed in the IPython Shell for you to explore.\n",
        "```\n",
        "oil\n",
        "        Date  Price\n",
        "0 1970-01-01   3.35\n",
        "1 1970-02-01   3.35\n",
        "2 1970-03-01   3.35\n",
        "3 1970-04-01   3.35\n",
        "4 1970-05-01   3.35\n",
        "\n",
        "auto\n",
        "    mpg  cyl  displ   hp  weight  accel         yr origin                       name\n",
        "0  18.0    8  307.0  130    3504   12.0 1970-01-01     US  chevrolet chevelle malibu\n",
        "1  15.0    8  350.0  165    3693   11.5 1970-01-01     US          buick skylark 320\n",
        "2  18.0    8  318.0  150    3436   11.0 1970-01-01     US         plymouth satellite\n",
        "3  16.0    8  304.0  150    3433   12.0 1970-01-01     US              amc rebel sst\n",
        "4  17.0    8  302.0  140    3449   10.5 1970-01-01     US                ford torino\n",
        "```\n",
        "These datasets will align such that the first price of the year will be broadcast into the rows of the automobiles DataFrame. This is considered correct since by the start of any given year, most automobiles for that year will have already been manufactured.\n",
        "\n",
        "You'll then inspect the merged DataFrame, resample by year and compute the mean 'Price' and 'mpg'. You should be able to see a trend in these two columns, that you can confirm by computing the Pearson correlation between resampled 'Price' and 'mpg'.\n",
        "\n",
        "- Merge auto and oil using pd.merge_asof() with left_on='yr' and right_on='Date'. Store the result as merged.\n",
        "- Print the tail of merged. This has been done for you.\n",
        "- Resample merged using 'A' (annual frequency), and on='Date'. Select [['mpg','Price']] and aggregate the mean. Store the result as yearly.\n",
        "- Hit Submit Answer to examine the contents of yearly and yearly.corr(), which shows the Pearson correlation between the resampled 'Price' and 'mpg'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_FqEYyHY1axn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Merge auto and oil: merged\n",
        "merged = pd.merge_asof(auto,oil,left_on='yr',right_on='Date')\n",
        "\n",
        "# Print the tail of merged\n",
        "print(merged.tail())\n",
        "\n",
        "# Resample merged: yearly\n",
        "yearly = merged.resample('A',on='Date')[['mpg','Price']].mean()\n",
        "\n",
        "# Print yearly\n",
        "print(yearly)\n",
        "\n",
        "# print yearly.corr()\n",
        "print(yearly.corr())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUvOGfNa7-2b",
        "colab_type": "text"
      },
      "source": [
        "```\n",
        "          mpg  cyl  displ  hp  weight  ...         yr  origin             name       Date  Price\n",
        "    387  27.0    4  140.0  86    2790  ... 1982-01-01      US  ford mustang gl 1982-01-01  33.85\n",
        "    388  44.0    4   97.0  52    2130  ... 1982-01-01  Europe        vw pickup 1982-01-01  33.85\n",
        "    389  32.0    4  135.0  84    2295  ... 1982-01-01      US    dodge rampage 1982-01-01  33.85\n",
        "    390  28.0    4  120.0  79    2625  ... 1982-01-01      US      ford ranger 1982-01-01  33.85\n",
        "    391  31.0    4  119.0  82    2720  ... 1982-01-01      US       chevy s-10 1982-01-01  33.85\n",
        "    \n",
        "    [5 rows x 11 columns]\n",
        "                      mpg  Price\n",
        "    Date                        \n",
        "    1970-12-31  17.689655   3.35\n",
        "    1971-12-31  21.111111   3.56\n",
        "    1972-12-31  18.714286   3.56\n",
        "    1973-12-31  17.100000   3.56\n",
        "    1974-12-31  22.769231  10.11\n",
        "    1975-12-31  20.266667  11.16\n",
        "    1976-12-31  21.573529  11.16\n",
        "    1977-12-31  23.375000  13.90\n",
        "    1978-12-31  24.061111  14.85\n",
        "    1979-12-31  25.093103  14.85\n",
        "    1980-12-31  33.803704  32.50\n",
        "    1981-12-31  30.185714  38.00\n",
        "    1982-12-31  32.000000  33.85\n",
        "                mpg     Price\n",
        "    mpg    1.000000  0.948677\n",
        "    Price  0.948677  1.000000\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7uhBpzr_mFj",
        "colab_type": "text"
      },
      "source": [
        "The expanding mean provides a way to see this down each column. It is the value of the mean with all the data available up to that point in time. If you are interested in learning more about pandas' expanding transformations, this section of the pandas documentation has additional information."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-q-YVix8F9h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}