{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Machine-learning-with-the-experts-school-budgets.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "D_2u-UUvgsD_"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUYzWJ_RrM6F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1sEGj-B3f1H",
        "colab_type": "text"
      },
      "source": [
        "### EDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTlIyMCJrETB",
        "colab_type": "code",
        "outputId": "d62f27d5-94d0-444b-d39e-551e8a39e134",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        }
      },
      "source": [
        "url = 'https://s3.amazonaws.com/drivendata/data/4/public/81e8f2de-9915-4934-b9ae-9705685c9d50.csv'\n",
        "url2 = 'https://s3.amazonaws.com/drivendata/data/4/public/d0fcd6d3-5bc5-4869-b4e6-d12ecb2ff517.csv'\n",
        "url3 = 'https://s3.amazonaws.com/drivendata/data/4/public/SubmissionFormat.csv'\n",
        "train = pd.read_csv(url,index_col=0)\n",
        "test = pd.read_csv(url2,index_col=0)\n",
        "sub = pd.read_csv(url3,index_col=0)\n",
        "train.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (5,11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Function</th>\n",
              "      <th>Use</th>\n",
              "      <th>Sharing</th>\n",
              "      <th>Reporting</th>\n",
              "      <th>Student_Type</th>\n",
              "      <th>Position_Type</th>\n",
              "      <th>Object_Type</th>\n",
              "      <th>Pre_K</th>\n",
              "      <th>Operating_Status</th>\n",
              "      <th>Object_Description</th>\n",
              "      <th>Text_2</th>\n",
              "      <th>SubFund_Description</th>\n",
              "      <th>Job_Title_Description</th>\n",
              "      <th>Text_3</th>\n",
              "      <th>Text_4</th>\n",
              "      <th>Sub_Object_Description</th>\n",
              "      <th>Location_Description</th>\n",
              "      <th>FTE</th>\n",
              "      <th>Function_Description</th>\n",
              "      <th>Facility_or_Department</th>\n",
              "      <th>Position_Extra</th>\n",
              "      <th>Total</th>\n",
              "      <th>Program_Description</th>\n",
              "      <th>Fund_Description</th>\n",
              "      <th>Text_1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>134338</th>\n",
              "      <td>Teacher Compensation</td>\n",
              "      <td>Instruction</td>\n",
              "      <td>School Reported</td>\n",
              "      <td>School</td>\n",
              "      <td>NO_LABEL</td>\n",
              "      <td>Teacher</td>\n",
              "      <td>NO_LABEL</td>\n",
              "      <td>NO_LABEL</td>\n",
              "      <td>PreK-12 Operating</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Teacher-Elementary</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>KINDERGARTEN</td>\n",
              "      <td>50471.810</td>\n",
              "      <td>KINDERGARTEN</td>\n",
              "      <td>General Fund</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>206341</th>\n",
              "      <td>NO_LABEL</td>\n",
              "      <td>NO_LABEL</td>\n",
              "      <td>NO_LABEL</td>\n",
              "      <td>NO_LABEL</td>\n",
              "      <td>NO_LABEL</td>\n",
              "      <td>NO_LABEL</td>\n",
              "      <td>NO_LABEL</td>\n",
              "      <td>NO_LABEL</td>\n",
              "      <td>Non-Operating</td>\n",
              "      <td>CONTRACTOR SERVICES</td>\n",
              "      <td>BOND EXPENDITURES</td>\n",
              "      <td>BUILDING FUND</td>\n",
              "      <td>(blank)</td>\n",
              "      <td>Regular</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>RGN  GOB</td>\n",
              "      <td>NaN</td>\n",
              "      <td>UNDESIGNATED</td>\n",
              "      <td>3477.860</td>\n",
              "      <td>BUILDING IMPROVEMENT SERVICES</td>\n",
              "      <td>NaN</td>\n",
              "      <td>BUILDING IMPROVEMENT SERVICES</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>326408</th>\n",
              "      <td>Teacher Compensation</td>\n",
              "      <td>Instruction</td>\n",
              "      <td>School Reported</td>\n",
              "      <td>School</td>\n",
              "      <td>Unspecified</td>\n",
              "      <td>Teacher</td>\n",
              "      <td>Base Salary/Compensation</td>\n",
              "      <td>Non PreK</td>\n",
              "      <td>PreK-12 Operating</td>\n",
              "      <td>Personal Services - Teachers</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>TCHER 2ND GRADE</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Regular Instruction</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>TEACHER</td>\n",
              "      <td>62237.130</td>\n",
              "      <td>Instruction - Regular</td>\n",
              "      <td>General Purpose School</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>364634</th>\n",
              "      <td>Substitute Compensation</td>\n",
              "      <td>Instruction</td>\n",
              "      <td>School Reported</td>\n",
              "      <td>School</td>\n",
              "      <td>Unspecified</td>\n",
              "      <td>Substitute</td>\n",
              "      <td>Benefits</td>\n",
              "      <td>NO_LABEL</td>\n",
              "      <td>PreK-12 Operating</td>\n",
              "      <td>EMPLOYEE BENEFITS</td>\n",
              "      <td>TEACHER SUBS</td>\n",
              "      <td>GENERAL FUND</td>\n",
              "      <td>Teacher, Short Term Sub</td>\n",
              "      <td>Regular</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>UNALLOC BUDGETS/SCHOOLS</td>\n",
              "      <td>NaN</td>\n",
              "      <td>PROFESSIONAL-INSTRUCTIONAL</td>\n",
              "      <td>22.300</td>\n",
              "      <td>GENERAL MIDDLE/JUNIOR HIGH SCH</td>\n",
              "      <td>NaN</td>\n",
              "      <td>REGULAR INSTRUCTION</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47683</th>\n",
              "      <td>Substitute Compensation</td>\n",
              "      <td>Instruction</td>\n",
              "      <td>School Reported</td>\n",
              "      <td>School</td>\n",
              "      <td>Unspecified</td>\n",
              "      <td>Teacher</td>\n",
              "      <td>Substitute Compensation</td>\n",
              "      <td>NO_LABEL</td>\n",
              "      <td>PreK-12 Operating</td>\n",
              "      <td>TEACHER COVERAGE FOR TEACHER</td>\n",
              "      <td>TEACHER SUBS</td>\n",
              "      <td>GENERAL FUND</td>\n",
              "      <td>Teacher, Secondary (High)</td>\n",
              "      <td>Alternative</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NON-PROJECT</td>\n",
              "      <td>NaN</td>\n",
              "      <td>PROFESSIONAL-INSTRUCTIONAL</td>\n",
              "      <td>54.166</td>\n",
              "      <td>GENERAL HIGH SCHOOL EDUCATION</td>\n",
              "      <td>NaN</td>\n",
              "      <td>REGULAR INSTRUCTION</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                       Function  ...                         Text_1\n",
              "134338     Teacher Compensation  ...                            NaN\n",
              "206341                 NO_LABEL  ...  BUILDING IMPROVEMENT SERVICES\n",
              "326408     Teacher Compensation  ...                            NaN\n",
              "364634  Substitute Compensation  ...            REGULAR INSTRUCTION\n",
              "47683   Substitute Compensation  ...            REGULAR INSTRUCTION\n",
              "\n",
              "[5 rows x 25 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMXWG_QMrWjA",
        "colab_type": "code",
        "outputId": "c5561abd-cd43-45c2-eeba-0c2611acd8da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        }
      },
      "source": [
        "test.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Object_Description</th>\n",
              "      <th>Program_Description</th>\n",
              "      <th>SubFund_Description</th>\n",
              "      <th>Job_Title_Description</th>\n",
              "      <th>Facility_or_Department</th>\n",
              "      <th>Sub_Object_Description</th>\n",
              "      <th>Location_Description</th>\n",
              "      <th>FTE</th>\n",
              "      <th>Function_Description</th>\n",
              "      <th>Position_Extra</th>\n",
              "      <th>Text_4</th>\n",
              "      <th>Total</th>\n",
              "      <th>Text_2</th>\n",
              "      <th>Text_3</th>\n",
              "      <th>Fund_Description</th>\n",
              "      <th>Text_1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>180042</th>\n",
              "      <td>Student Meals/Room/Other</td>\n",
              "      <td>Basic Educational Services</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Line Item that is paid with Campus' money</td>\n",
              "      <td>School</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Instruction</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3999.910000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>General Fund</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28872</th>\n",
              "      <td>Extra Duty/Signing Bonus Pay</td>\n",
              "      <td>Undistributed</td>\n",
              "      <td>NaN</td>\n",
              "      <td>CHEERLEADER DIR</td>\n",
              "      <td>NaN</td>\n",
              "      <td>General</td>\n",
              "      <td>School</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Cocurricular &amp; Extra Curricular Activities</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3447.320213</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>General Fund</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>186915</th>\n",
              "      <td>Professional Salaries</td>\n",
              "      <td>Bilingual Education</td>\n",
              "      <td>NaN</td>\n",
              "      <td>T-EL 1ST BIL</td>\n",
              "      <td>NaN</td>\n",
              "      <td>General</td>\n",
              "      <td>School</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Instruction</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>52738.780869</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>General Fund</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>412396</th>\n",
              "      <td>Professional Salaries</td>\n",
              "      <td>Bilingual Education</td>\n",
              "      <td>NaN</td>\n",
              "      <td>T-EL 2ND BIL</td>\n",
              "      <td>NaN</td>\n",
              "      <td>General</td>\n",
              "      <td>School</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Instruction</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>69729.263191</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>General Fund</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>427740</th>\n",
              "      <td>Salaries for Support Personnel</td>\n",
              "      <td>Undistributed</td>\n",
              "      <td>NaN</td>\n",
              "      <td>CLERK III- SCH</td>\n",
              "      <td>NaN</td>\n",
              "      <td>General</td>\n",
              "      <td>School</td>\n",
              "      <td>1.0</td>\n",
              "      <td>School Leadership</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>29492.834215</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>General Fund</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                    Object_Description  ... Text_1\n",
              "180042        Student Meals/Room/Other  ...    NaN\n",
              "28872     Extra Duty/Signing Bonus Pay  ...    NaN\n",
              "186915           Professional Salaries  ...    NaN\n",
              "412396           Professional Salaries  ...    NaN\n",
              "427740  Salaries for Support Personnel  ...    NaN\n",
              "\n",
              "[5 rows x 16 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jk7QjuznsAFe",
        "colab_type": "code",
        "outputId": "7322c126-7d52-4a83-dad2-addeae6e9426",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        }
      },
      "source": [
        "sub.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Function__Aides Compensation</th>\n",
              "      <th>Function__Career &amp; Academic Counseling</th>\n",
              "      <th>Function__Communications</th>\n",
              "      <th>Function__Curriculum Development</th>\n",
              "      <th>Function__Data Processing &amp; Information Services</th>\n",
              "      <th>Function__Development &amp; Fundraising</th>\n",
              "      <th>Function__Enrichment</th>\n",
              "      <th>Function__Extended Time &amp; Tutoring</th>\n",
              "      <th>Function__Facilities &amp; Maintenance</th>\n",
              "      <th>Function__Facilities Planning</th>\n",
              "      <th>Function__Finance, Budget, Purchasing &amp; Distribution</th>\n",
              "      <th>Function__Food Services</th>\n",
              "      <th>Function__Governance</th>\n",
              "      <th>Function__Human Resources</th>\n",
              "      <th>Function__Instructional Materials &amp; Supplies</th>\n",
              "      <th>Function__Insurance</th>\n",
              "      <th>Function__Legal</th>\n",
              "      <th>Function__Library &amp; Media</th>\n",
              "      <th>Function__NO_LABEL</th>\n",
              "      <th>Function__Other Compensation</th>\n",
              "      <th>Function__Other Non-Compensation</th>\n",
              "      <th>Function__Parent &amp; Community Relations</th>\n",
              "      <th>Function__Physical Health &amp; Services</th>\n",
              "      <th>Function__Professional Development</th>\n",
              "      <th>Function__Recruitment</th>\n",
              "      <th>Function__Research &amp; Accountability</th>\n",
              "      <th>Function__School Administration</th>\n",
              "      <th>Function__School Supervision</th>\n",
              "      <th>Function__Security &amp; Safety</th>\n",
              "      <th>Function__Social &amp; Emotional</th>\n",
              "      <th>Function__Special Population Program Management &amp; Support</th>\n",
              "      <th>Function__Student Assignment</th>\n",
              "      <th>Function__Student Transportation</th>\n",
              "      <th>Function__Substitute Compensation</th>\n",
              "      <th>Function__Teacher Compensation</th>\n",
              "      <th>Function__Untracked Budget Set-Aside</th>\n",
              "      <th>Function__Utilities</th>\n",
              "      <th>Object_Type__Base Salary/Compensation</th>\n",
              "      <th>Object_Type__Benefits</th>\n",
              "      <th>Object_Type__Contracted Services</th>\n",
              "      <th>...</th>\n",
              "      <th>Position_Type__Other</th>\n",
              "      <th>Position_Type__Physical Therapist</th>\n",
              "      <th>Position_Type__Principal</th>\n",
              "      <th>Position_Type__Psychologist</th>\n",
              "      <th>Position_Type__School Monitor/Security</th>\n",
              "      <th>Position_Type__Sec/Clerk/Other Admin</th>\n",
              "      <th>Position_Type__Social Worker</th>\n",
              "      <th>Position_Type__Speech Therapist</th>\n",
              "      <th>Position_Type__Substitute</th>\n",
              "      <th>Position_Type__TA</th>\n",
              "      <th>Position_Type__Teacher</th>\n",
              "      <th>Position_Type__Vice Principal</th>\n",
              "      <th>Pre_K__NO_LABEL</th>\n",
              "      <th>Pre_K__Non PreK</th>\n",
              "      <th>Pre_K__PreK</th>\n",
              "      <th>Reporting__NO_LABEL</th>\n",
              "      <th>Reporting__Non-School</th>\n",
              "      <th>Reporting__School</th>\n",
              "      <th>Sharing__Leadership &amp; Management</th>\n",
              "      <th>Sharing__NO_LABEL</th>\n",
              "      <th>Sharing__School Reported</th>\n",
              "      <th>Sharing__School on Central Budgets</th>\n",
              "      <th>Sharing__Shared Services</th>\n",
              "      <th>Student_Type__Alternative</th>\n",
              "      <th>Student_Type__At Risk</th>\n",
              "      <th>Student_Type__ELL</th>\n",
              "      <th>Student_Type__Gifted</th>\n",
              "      <th>Student_Type__NO_LABEL</th>\n",
              "      <th>Student_Type__Poverty</th>\n",
              "      <th>Student_Type__PreK</th>\n",
              "      <th>Student_Type__Special Education</th>\n",
              "      <th>Student_Type__Unspecified</th>\n",
              "      <th>Use__Business Services</th>\n",
              "      <th>Use__ISPD</th>\n",
              "      <th>Use__Instruction</th>\n",
              "      <th>Use__Leadership</th>\n",
              "      <th>Use__NO_LABEL</th>\n",
              "      <th>Use__O&amp;M</th>\n",
              "      <th>Use__Pupil Services &amp; Enrichment</th>\n",
              "      <th>Use__Untracked Budget Set-Aside</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>180042</th>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>...</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28872</th>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>...</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>186915</th>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>...</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>412396</th>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>...</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>427740</th>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>...</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.125</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 104 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        Function__Aides Compensation  ...  Use__Untracked Budget Set-Aside\n",
              "180042                      0.027027  ...                            0.125\n",
              "28872                       0.027027  ...                            0.125\n",
              "186915                      0.027027  ...                            0.125\n",
              "412396                      0.027027  ...                            0.125\n",
              "427740                      0.027027  ...                            0.125\n",
              "\n",
              "[5 rows x 104 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WsHnkefjsTqM",
        "colab_type": "code",
        "outputId": "1ecbed21-c000-4c6e-a9b0-ed6b15a7017e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(sub.columns)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "104"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45qdVMnHskxI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2NThB7cs6Rl",
        "colab_type": "code",
        "outputId": "c6feb607-5d3e-41b0-bb83-f0ad29b0d9d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "source": [
        "df.describe()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>FTE</th>\n",
              "      <th>Total</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>126071.000000</td>\n",
              "      <td>3.957220e+05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.426794</td>\n",
              "      <td>1.310586e+04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.573576</td>\n",
              "      <td>3.682254e+05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>-0.087551</td>\n",
              "      <td>-8.746631e+07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.000792</td>\n",
              "      <td>7.379770e+01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.130927</td>\n",
              "      <td>4.612300e+02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>3.652662e+03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>46.800000</td>\n",
              "      <td>1.297000e+08</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 FTE         Total\n",
              "count  126071.000000  3.957220e+05\n",
              "mean        0.426794  1.310586e+04\n",
              "std         0.573576  3.682254e+05\n",
              "min        -0.087551 -8.746631e+07\n",
              "25%         0.000792  7.379770e+01\n",
              "50%         0.130927  4.612300e+02\n",
              "75%         1.000000  3.652662e+03\n",
              "max        46.800000  1.297000e+08"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gh_ypvdEs2sI",
        "colab_type": "code",
        "outputId": "980eb030-62dd-4511-d018-ea889a1234dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        }
      },
      "source": [
        "# Create the histogram\n",
        "plt.hist(df['FTE'].dropna())\n",
        "\n",
        "# Add title and labels\n",
        "plt.title('Distribution of %full-time \\n employee works')\n",
        "plt.xlabel('% of full-time')\n",
        "plt.ylabel('num employees')\n",
        "\n",
        "# Display the histogram\n",
        "plt.show()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZsAAAElCAYAAAAyWE/9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nki1ZKrgzAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFEKjzsYuDjp",
        "colab_type": "code",
        "outputId": "639b73c3-605d-4e4e-abf1-4578d44227cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "np.sum(df['FTE']<=1)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "123003"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPTdkdF3s8ds",
        "colab_type": "code",
        "outputId": "c4a3aa98-c4da-4ff5-82d5-c1fec2d5993e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        }
      },
      "source": [
        "# Create the histogram\n",
        "plt.hist(df[df['FTE']<=1]['FTE'].dropna())\n",
        "\n",
        "# Add title and labels\n",
        "plt.title('Distribution of %full-time \\n employee works')\n",
        "plt.xlabel('% of full-time')\n",
        "plt.ylabel('num employees')\n",
        "\n",
        "# Display the histogram\n",
        "plt.show()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZUAAAElCAYAAAAskX9OAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMk4qZmZWmf8PVUQS09sk0U4AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xw_4-49Itx8a",
        "colab_type": "code",
        "outputId": "97b4bfc9-0888-4601-f360-326b0a417b78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "df.dtypes.value_counts()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "object     23\n",
              "float64     2\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6xQAbPH3qoD",
        "colab_type": "text"
      },
      "source": [
        "https://www.drivendata.org/competitions/4/box-plots-for-education/page/15/#labels_list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fT56CImw3Q1C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LABELS= ['Function',\n",
        " 'Use',\n",
        " 'Sharing',\n",
        " 'Reporting',\n",
        " 'Student_Type',\n",
        " 'Position_Type',\n",
        " 'Object_Type',\n",
        " 'Pre_K',\n",
        " 'Operating_Status']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2icHd263n3L",
        "colab_type": "code",
        "outputId": "803d48c3-8cc6-4427-cef5-9d39c75ea34c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "df[LABELS].dtypes"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Function            object\n",
              "Use                 object\n",
              "Sharing             object\n",
              "Reporting           object\n",
              "Student_Type        object\n",
              "Position_Type       object\n",
              "Object_Type         object\n",
              "Pre_K               object\n",
              "Operating_Status    object\n",
              "dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j76nV6Ju3ynJ",
        "colab_type": "text"
      },
      "source": [
        "Note: .astype() only works on a pandas Series. Since you are working with a pandas DataFrame, you'll need to use the .apply() method and provide a lambda function called categorize_label that applies .astype() to each column, x"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9Tr7gzE3uTb",
        "colab_type": "code",
        "outputId": "c255a481-3b76-4b6d-a257-6ae6a5d088bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "# Define the lambda function: categorize_label\n",
        "categorize_label = lambda x: x.astype('category')\n",
        "\n",
        "# Convert df[LABELS] to a categorical type\n",
        "df[LABELS] = df[LABELS].apply(categorize_label,axis=0)\n",
        "\n",
        "# Print the converted dtypes\n",
        "print(df[LABELS].dtypes)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Function            category\n",
            "Use                 category\n",
            "Sharing             category\n",
            "Reporting           category\n",
            "Student_Type        category\n",
            "Position_Type       category\n",
            "Object_Type         category\n",
            "Pre_K               category\n",
            "Operating_Status    category\n",
            "dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hsumMVP3_8t",
        "colab_type": "code",
        "outputId": "f6e8f894-cba0-4f4a-a99e-5f2dae37091a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        }
      },
      "source": [
        "# Import matplotlib.pyplot\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Calculate number of unique values for each label: num_unique_labels\n",
        "num_unique_labels = df[LABELS].apply(pd.Series.nunique)\n",
        "\n",
        "# Plot number of unique values for each label\n",
        "num_unique_labels.plot(kind='bar')\n",
        "\n",
        "# Label the axes\n",
        "plt.xlabel('Labels')\n",
        "plt.ylabel('Number of unique values')\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAFWCAYAAABkVZqwAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/wP+nQyTNYu/AgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqrCsnpu4ZiS",
        "colab_type": "code",
        "outputId": "264f0fb5-bbed-44eb-883d-a40a58f65e19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "num_unique_labels"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Function            37\n",
              "Use                  8\n",
              "Sharing              5\n",
              "Reporting            3\n",
              "Student_Type         9\n",
              "Position_Type       25\n",
              "Object_Type         11\n",
              "Pre_K                3\n",
              "Operating_Status     3\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fLFbyYR4f_d",
        "colab_type": "code",
        "outputId": "c31f6d99-4152-4b6c-f103-9d8f80f24523",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "np.sum(num_unique_labels) #total number of categories"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "104"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgsmrlUP3lLZ",
        "colab_type": "text"
      },
      "source": [
        "### helpers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCVdt-9H4iaq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_log_loss(predicted, actual,eps=1e-14):\n",
        "  ## for binary classification\n",
        "  predicted = mp.clip(predicted, eps, 1-eps)\n",
        "  loss = -1 * np.mean(actual*np.log(predicted)\n",
        "          + (1-actual)*np.log(1-predicted))\n",
        "  return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0x7xpcJrJke",
        "colab_type": "text"
      },
      "source": [
        "```\n",
        "    Log loss, correct and confident: 0.05129329438755058\n",
        "    Log loss, correct and not confident: 0.4307829160924542\n",
        "    Log loss, wrong and not confident: 1.049822124498678\n",
        "    Log loss, wrong and confident: 2.9957322735539904\n",
        "    Log loss, actual labels: 9.99200722162646e-15\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhCVRjjVvkzK",
        "colab_type": "text"
      },
      "source": [
        "The first step is to split the data into a training set and a test set. Some labels don't occur very often, but we want to make sure that they appear in both the training and the test sets. We provide a function that will make sure at least min_count examples of each label appear in each split: multilabel_train_test_split."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWWgMQBLqD56",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NUMERIC_COLUMNS = ['FTE', 'Total']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNcYNPpUwWqf",
        "colab_type": "text"
      },
      "source": [
        "https://github.com/drivendataorg/box-plots-sklearn/blob/master/src/data/multilabel.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApLn9zuYvu1f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from warnings import warn\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def multilabel_sample(y, size=1000, min_count=5, seed=None):\n",
        "    \"\"\" Takes a matrix of binary labels `y` and returns\n",
        "        the indices for a sample of size `size` if\n",
        "        `size` > 1 or `size` * len(y) if size =< 1.\n",
        "        The sample is guaranteed to have > `min_count` of\n",
        "        each label.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if (np.unique(y).astype(int) != np.array([0, 1])).any():\n",
        "            raise ValueError()\n",
        "    except (TypeError, ValueError):\n",
        "        raise ValueError('multilabel_sample only works with binary indicator matrices')\n",
        "\n",
        "    if (y.sum(axis=0) < min_count).any():\n",
        "        raise ValueError('Some classes do not have enough examples. Change min_count if necessary.')\n",
        "\n",
        "    if size <= 1:\n",
        "        size = np.floor(y.shape[0] * size)\n",
        "\n",
        "    if y.shape[1] * min_count > size:\n",
        "        msg = \"Size less than number of columns * min_count, returning {} items instead of {}.\"\n",
        "        warn(msg.format(y.shape[1] * min_count, size))\n",
        "        size = y.shape[1] * min_count\n",
        "\n",
        "    rng = np.random.RandomState(seed if seed is not None else np.random.randint(1))\n",
        "\n",
        "    if isinstance(y, pd.DataFrame):\n",
        "        choices = y.index\n",
        "        y = y.values\n",
        "    else:\n",
        "        choices = np.arange(y.shape[0])\n",
        "\n",
        "    sample_idxs = np.array([], dtype=choices.dtype)\n",
        "\n",
        "    # first, guarantee > min_count of each label\n",
        "    for j in range(y.shape[1]):\n",
        "        label_choices = choices[y[:, j] == 1]\n",
        "        label_idxs_sampled = rng.choice(label_choices, size=min_count, replace=False)\n",
        "        sample_idxs = np.concatenate([label_idxs_sampled, sample_idxs])\n",
        "\n",
        "    sample_idxs = np.unique(sample_idxs)\n",
        "\n",
        "    # now that we have at least min_count of each, we can just random sample\n",
        "    sample_count = int(size - sample_idxs.shape[0])\n",
        "\n",
        "    # get sample_count indices from remaining choices\n",
        "    remaining_choices = np.setdiff1d(choices, sample_idxs)\n",
        "    remaining_sampled = rng.choice(remaining_choices,\n",
        "                                   size=sample_count,\n",
        "                                   replace=False)\n",
        "\n",
        "    return np.concatenate([sample_idxs, remaining_sampled])\n",
        "\n",
        "\n",
        "def multilabel_sample_dataframe(df, labels, size, min_count=5, seed=None):\n",
        "    \"\"\" Takes a dataframe `df` and returns a sample of size `size` where all\n",
        "        classes in the binary matrix `labels` are represented at\n",
        "        least `min_count` times.\n",
        "    \"\"\"\n",
        "    idxs = multilabel_sample(labels, size=size, min_count=min_count, seed=seed)\n",
        "    return df.loc[idxs]\n",
        "\n",
        "\n",
        "def multilabel_train_test_split(X, Y, size, min_count=5, seed=None):\n",
        "    \"\"\" Takes a features matrix `X` and a label matrix `Y` and\n",
        "        returns (X_train, X_test, Y_train, Y_test) where all\n",
        "        classes in Y are represented at least `min_count` times.\n",
        "    \"\"\"\n",
        "    index = Y.index if isinstance(Y, pd.DataFrame) else np.arange(Y.shape[0])\n",
        "\n",
        "    test_set_idxs = multilabel_sample(Y, size=size, min_count=min_count, seed=seed)\n",
        "    train_set_idxs = np.setdiff1d(index, test_set_idxs)\n",
        "\n",
        "    test_set_mask = index.isin(test_set_idxs)\n",
        "    train_set_mask = ~test_set_mask\n",
        "\n",
        "    return (X[train_set_mask], X[test_set_mask], Y[train_set_mask], Y[test_set_mask])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9jv-koJykp-",
        "colab_type": "text"
      },
      "source": [
        "### With only numeric columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6PvVzQYwXZT",
        "colab_type": "code",
        "outputId": "cc333abc-c0a3-4e9e-d517-3db8a1429402",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        }
      },
      "source": [
        "# Create the new DataFrame: numeric_data_only\n",
        "numeric_data_only = df[NUMERIC_COLUMNS].fillna(-1000)\n",
        "\n",
        "# Get labels and convert to dummy variables: label_dummies\n",
        "label_dummies = pd.get_dummies(df[LABELS])\n",
        "\n",
        "# Create training and test sets\n",
        "X_train, X_test, y_train, y_test = multilabel_train_test_split(numeric_data_only,label_dummies,size=0.2,seed=123)\n",
        "\n",
        "# Print the info\n",
        "print(\"X_train info:\")\n",
        "print(X_train.info())\n",
        "print(\"\\nX_test info:\")  \n",
        "print(X_test.info())\n",
        "print(\"\\ny_train info:\")  \n",
        "print(y_train.info())\n",
        "print(\"\\ny_test info:\")  \n",
        "print(y_test.info()) "
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 320222 entries, 134338 to 415831\n",
            "Data columns (total 2 columns):\n",
            "FTE      320222 non-null float64\n",
            "Total    320222 non-null float64\n",
            "dtypes: float64(2)\n",
            "memory usage: 7.3 MB\n",
            "None\n",
            "\n",
            "X_test info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 80055 entries, 206341 to 72072\n",
            "Data columns (total 2 columns):\n",
            "FTE      80055 non-null float64\n",
            "Total    80055 non-null float64\n",
            "dtypes: float64(2)\n",
            "memory usage: 1.8 MB\n",
            "None\n",
            "\n",
            "y_train info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 320222 entries, 134338 to 415831\n",
            "Columns: 104 entries, Function_Aides Compensation to Operating_Status_PreK-12 Operating\n",
            "dtypes: uint8(104)\n",
            "memory usage: 34.2 MB\n",
            "None\n",
            "\n",
            "y_test info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 80055 entries, 206341 to 72072\n",
            "Columns: 104 entries, Function_Aides Compensation to Operating_Status_PreK-12 Operating\n",
            "dtypes: uint8(104)\n",
            "memory usage: 8.6 MB\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGB_9tCtynBl",
        "colab_type": "code",
        "outputId": "eb00bcf3-8c01-4760-aa13-1d12ac4d579e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        }
      },
      "source": [
        "# Import classifiers\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "\n",
        "# Create the DataFrame: numeric_data_only\n",
        "numeric_data_only = df[NUMERIC_COLUMNS].fillna(-1000)\n",
        "\n",
        "# Get labels and convert to dummy variables: label_dummies\n",
        "label_dummies = pd.get_dummies(df[LABELS])\n",
        "\n",
        "# Create training and test sets\n",
        "X_train, X_test, y_train, y_test = multilabel_train_test_split(numeric_data_only,label_dummies,size=0.2, seed=123)\n",
        "\n",
        "# Instantiate the classifier: clf\n",
        "clf = OneVsRestClassifier(LogisticRegression(solver='liblinear'))\n",
        "##one vs all: make separate models for each label\n",
        "\n",
        "# Fit the classifier to the training data\n",
        "clf.fit(X_train,y_train)\n",
        "\n",
        "# Print the accuracy\n",
        "print(\"Accuracy: {}\".format(clf.score(X_test, y_test)))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tai-2T-hzwI_",
        "colab_type": "text"
      },
      "source": [
        "Ok! The good news is that your workflow didn't cause any errors. The bad news is that your model scored the lowest possible accuracy: 0.0! But hey, you just threw away ALL of the text data in the budget. Later, you won't. Before you add the text data, let's see how the model does when scored by log loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cw5CbI6szr3r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "holdout = test\n",
        "# Generate predictions: predictions\n",
        "predictions = clf.predict_proba(holdout[NUMERIC_COLUMNS].fillna(-1000))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bQVlNWK1iNm",
        "colab_type": "text"
      },
      "source": [
        "When interpreting your log loss score, keep in mind that the score will change based on the number of samples tested. To get a sense of how this very basic model performs, compare your score to the DrivenData benchmark model performance: 2.0455, which merely submitted uniform probabilities for each class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5ylI9ET1b-E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generate predictions: predictions\n",
        "predictions = clf.predict_proba(holdout[NUMERIC_COLUMNS].fillna(-1000))\n",
        "\n",
        "# Format predictions in DataFrame: prediction_df\n",
        "prediction_df = pd.DataFrame(columns=pd.get_dummies(df[LABELS]).columns,\n",
        "                             index=holdout.index,\n",
        "                             data=predictions)\n",
        "\n",
        "\n",
        "# Save prediction_df to csv\n",
        "prediction_df.to_csv('predictions.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hw7A0HqS2Ddo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Submit the predictions for scoring: score\n",
        "score = score_submission(pred_path='predictions.csv')\n",
        "\n",
        "# Print score\n",
        "print('Your model, trained with numeric data only, yields logloss score: {}'.format(score))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnOMsO0P2Fgi",
        "colab_type": "text"
      },
      "source": [
        "```\n",
        "Your model, trained with numeric data only, yields logloss score: 1.9067227623381413\n",
        "```\n",
        "\n",
        "Incredible! Even though your basic model scored 0.0 accuracy, it nevertheless performs better than the benchmark score of 2.0455. You've now got the basics down and have made a first pass at this complicated supervised learning problem. It's time to step up your game and incorporate the text data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2F_mD_-G3aTZ",
        "colab_type": "text"
      },
      "source": [
        "### Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNsV3AX52K9e",
        "colab_type": "code",
        "outputId": "e853cda8-40ca-46be-d3fd-1c3c224a4d30",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Import CountVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Create the token pattern: TOKENS_ALPHANUMERIC\n",
        "TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)'\n",
        "\n",
        "# Fill missing values in df.Position_Extra\n",
        "df.Position_Extra.fillna('',inplace=True)\n",
        "\n",
        "# Instantiate the CountVectorizer: vec_alphanumeric\n",
        "vec_alphanumeric = CountVectorizer(token_pattern=TOKENS_ALPHANUMERIC)\n",
        "\n",
        "# Fit to the data\n",
        "vec_alphanumeric.fit(df.Position_Extra)\n",
        "\n",
        "# Print the number of tokens and first 15 tokens\n",
        "msg = \"There are {} tokens in Position_Extra if we split on non-alpha numeric\"\n",
        "print(msg.format(len(vec_alphanumeric.get_feature_names())))\n",
        "print(vec_alphanumeric.get_feature_names()[:15])"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 385 tokens in Position_Extra if we split on non-alpha numeric\n",
            "['1st', '2nd', '3rd', '4th', '56', '5th', '9th', 'a', 'ab', 'accountability', 'adaptive', 'addit', 'additional', 'adm', 'admin']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-IO5RDf24uHf",
        "colab_type": "text"
      },
      "source": [
        "In order to get a bag-of-words representation for all of the text data in our DataFrame, you must first convert the text data in each row of the DataFrame into a single string.\n",
        "\n",
        "In the previous exercise, this wasn't necessary because you only looked at one column of data, so each row was already just a single string. CountVectorizer expects each row to just be a single string, so in order to use all of the text columns, you'll need a method to turn a list of strings into a single string.\n",
        "In this exercise, you'll complete the function definition combine_text_columns(). When completed, this function will convert all training text data in your DataFrame to a single string per row that can be passed to the vectorizer object and made into a bag-of-words using the .fit_transform() method.\n",
        "\n",
        "Note that the function uses NUMERIC_COLUMNS and LABELS to determine which columns to drop. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6OleDUd4gHm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define combine_text_columns()\n",
        "def combine_text_columns(data_frame, to_drop=NUMERIC_COLUMNS + LABELS):\n",
        "    \"\"\" converts all text in each row of data_frame to single vector \"\"\"\n",
        "    \n",
        "    # Drop non-text columns that are in the df\n",
        "    to_drop = set(to_drop) & set(data_frame.columns.tolist())\n",
        "    text_data = data_frame.drop(to_drop,axis=1)\n",
        "    \n",
        "    # Replace nans with blanks\n",
        "    text_data.fillna('',inplace=True)\n",
        "    \n",
        "    # Join all text items in a row that have a space in between\n",
        "    return text_data.apply(lambda x: \" \".join(x), axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHUZdYkW5UMB",
        "colab_type": "code",
        "outputId": "68d39ff3-50fa-434e-acae-92e08be86a75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Import the CountVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Create the basic token pattern\n",
        "TOKENS_BASIC = '\\\\S+(?=\\\\s+)'\n",
        "\n",
        "# Create the alphanumeric token pattern\n",
        "TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)'\n",
        "\n",
        "# Instantiate basic CountVectorizer: vec_basic\n",
        "vec_basic = CountVectorizer(token_pattern=TOKENS_BASIC)\n",
        "\n",
        "# Instantiate alphanumeric CountVectorizer: vec_alphanumeric\n",
        "vec_alphanumeric = CountVectorizer(token_pattern=TOKENS_ALPHANUMERIC)\n",
        "\n",
        "# Create the text vector\n",
        "text_vector = combine_text_columns(df)\n",
        "\n",
        "# Fit and transform vec_basic\n",
        "vec_basic.fit_transform(text_vector)\n",
        "\n",
        "# Print number of tokens of vec_basic\n",
        "print(\"There are {} tokens in the dataset\".format(len(vec_basic.get_feature_names())))\n",
        "\n",
        "# Fit and transform vec_alphanumeric\n",
        "vec_alphanumeric.fit_transform(text_vector)\n",
        "\n",
        "# Print number of tokens of vec_alphanumeric\n",
        "print(\"There are {} alpha-numeric tokens in the dataset\".format(len(vec_alphanumeric.get_feature_names())))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 4757 tokens in the dataset\n",
            "There are 3284 alpha-numeric tokens in the dataset\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SvOn_cZRdZ7c",
        "colab_type": "text"
      },
      "source": [
        "By default, the imputer transformer replaces NaNs with the mean value of the column. That's a good enough imputation strategy for the sample data, so you won't need to pass anything extra to the imputer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_2u-UUvgsD_",
        "colab_type": "text"
      },
      "source": [
        "#### ON sample data (learning purposes)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnQn_pvv6CLu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "# Import other necessary modules\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "\n",
        "# Split and select numeric data only, no nans \n",
        "X_train, X_test, y_train, y_test = train_test_split(sample_df[['numeric']],\n",
        "                                                    pd.get_dummies(sample_df['label']), \n",
        "                                                    random_state=22)\n",
        "\n",
        "# Instantiate Pipeline object: pl\n",
        "pl = Pipeline([\n",
        "        ('clf', OneVsRestClassifier(LogisticRegression()))\n",
        "    ])\n",
        "\n",
        "# Fit the pipeline to the training data\n",
        "pl.fit(X_train, y_train)\n",
        "\n",
        "# Compute and print accuracy\n",
        "accuracy = pl.score(X_test, y_test)\n",
        "print(\"\\nAccuracy on sample data - numeric, no nans: \", accuracy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbP01YvogwYj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the Imputer object\n",
        "from sklearn.preprocessing import Imputer\n",
        "\n",
        "# Create training and test sets using only numeric data\n",
        "X_train, X_test, y_train, y_test = train_test_split(sample_df[['numeric', 'with_missing']],\n",
        "                                                    pd.get_dummies(sample_df['label']), \n",
        "                                                    random_state=456)\n",
        "\n",
        "# Insantiate Pipeline object: pl\n",
        "pl = Pipeline([\n",
        "        ('imp', Imputer()),\n",
        "        ('clf', OneVsRestClassifier(LogisticRegression()))\n",
        "    ])\n",
        "\n",
        "# Fit the pipeline to the training data\n",
        "pl.fit(X_train,y_train)\n",
        "\n",
        "# Compute and print accuracy\n",
        "accuracy = pl.score(X_test,y_test)\n",
        "print(\"\\nAccuracy on sample data - all numeric, incl nans: \", accuracy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUVQosw6gwV5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the CountVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Split out only the text data\n",
        "X_train, X_test, y_train, y_test = train_test_split(sample_df['text'],\n",
        "                                                    pd.get_dummies(sample_df['label']), \n",
        "                                                    random_state=456)\n",
        "\n",
        "# Instantiate Pipeline object: pl\n",
        "pl = Pipeline([\n",
        "        ('vec', CountVectorizer()),\n",
        "        ('clf', OneVsRestClassifier(LogisticRegression()))\n",
        "    ])\n",
        "\n",
        "# Fit to the training data\n",
        "pl.fit(X_train,y_train)\n",
        "\n",
        "# Compute and print accuracy\n",
        "accuracy = pl.score(X_test,y_test)\n",
        "print(\"\\nAccuracy on sample data - just text data: \", accuracy)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQold-Y7g3pN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import FunctionTransformer\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "\n",
        "# Obtain the text data: get_text_data\n",
        "get_text_data = FunctionTransformer(lambda x: x['text'], validate=False)\n",
        "\n",
        "# Obtain the numeric data: get_numeric_data\n",
        "get_numeric_data = FunctionTransformer(lambda x: x[['numeric','with_missing']], validate=False)\n",
        "\n",
        "# Fit and transform the text data: just_text_data\n",
        "just_text_data = get_text_data.fit_transform(sample_df)\n",
        "\n",
        "# Fit and transform the numeric data: just_numeric_data\n",
        "just_numeric_data = get_numeric_data.fit_transform(sample_df)\n",
        "\n",
        "# Print head to check results\n",
        "print('Text Data')\n",
        "print(just_text_data.head())\n",
        "print('\\nNumeric Data')\n",
        "print(just_numeric_data.head())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lm4uiWgrhByv",
        "colab_type": "text"
      },
      "source": [
        "you don't want to impute our text data, and you don't want to create a bag-of-words with our numeric data. Instead, you want to deal with these separately and then join the results together using FeatureUnion()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zq7O03oQg3lJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import FeatureUnion\n",
        "from sklearn.pipeline import FeatureUnion\n",
        "\n",
        "# Split using ALL data in sample_df\n",
        "X_train, X_test, y_train, y_test = train_test_split(sample_df[['numeric', 'with_missing', 'text']],\n",
        "                                                    pd.get_dummies(sample_df['label']), \n",
        "                                                    random_state=22)\n",
        "\n",
        "# Create a FeatureUnion with nested pipeline: process_and_join_features\n",
        "process_and_join_features = FeatureUnion(\n",
        "            transformer_list = [\n",
        "                ('numeric_features', Pipeline([\n",
        "                    ('selector', get_numeric_data),\n",
        "                    ('imputer', Imputer())\n",
        "                ])),\n",
        "                ('text_features', Pipeline([\n",
        "                    ('selector', get_text_data),\n",
        "                    ('vectorizer', CountVectorizer())\n",
        "                ]))\n",
        "             ]\n",
        "        )\n",
        "\n",
        "# Instantiate nested pipeline: pl\n",
        "pl = Pipeline([\n",
        "        ('union', process_and_join_features),\n",
        "        ('clf', OneVsRestClassifier(LogisticRegression()))\n",
        "    ])\n",
        "\n",
        "\n",
        "# Fit pl to the training data\n",
        "pl.fit(X_train, y_train)\n",
        "\n",
        "# Compute and print accuracy\n",
        "accuracy = pl.score(X_test, y_test)\n",
        "print(\"\\nAccuracy on sample data - all data: \", accuracy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FwJJJD9iyqk",
        "colab_type": "text"
      },
      "source": [
        "### In actual dataset, using all columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGlIoylmjdh_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import Imputer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from sklearn.pipeline import FeatureUnion"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ot_GwVYji18X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import FunctionTransformer\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "\n",
        "# Get the dummy encoding of the labels\n",
        "dummy_labels = pd.get_dummies(df[LABELS])\n",
        "\n",
        "# Get the columns that are features in the original df\n",
        "NON_LABELS = [c for c in df.columns if c not in LABELS]\n",
        "\n",
        "# Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = multilabel_train_test_split(df[NON_LABELS],\n",
        "                                                               dummy_labels,\n",
        "                                                               0.2, \n",
        "                                                               seed=123)\n",
        "\n",
        "# Preprocess the text data: get_text_data\n",
        "get_text_data = FunctionTransformer(combine_text_columns,validate=False)\n",
        "\n",
        "# Preprocess the numeric data: get_numeric_data\n",
        "get_numeric_data = FunctionTransformer(lambda x: x[NUMERIC_COLUMNS], validate=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmWSk3z1jQAj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "bcf8d1f8-f890-4778-aeb3-ed9dbc04532f"
      },
      "source": [
        "# Complete the pipeline: pl\n",
        "pl = Pipeline([\n",
        "        ('union', FeatureUnion(\n",
        "            transformer_list = [\n",
        "                ('numeric_features', Pipeline([\n",
        "                    ('selector', get_numeric_data),\n",
        "                    ('imputer',Imputer())\n",
        "                ])),\n",
        "                ('text_features', Pipeline([\n",
        "                    ('selector', get_text_data),\n",
        "                    ('vectorizer', CountVectorizer())\n",
        "                ]))\n",
        "             ]\n",
        "        )),\n",
        "        ('clf', OneVsRestClassifier(LogisticRegression()))\n",
        "    ])\n",
        "\n",
        "# Fit to the training data\n",
        "pl.fit(X_train,y_train)\n",
        "\n",
        "# Compute and print accuracy\n",
        "accuracy = pl.score(X_test, y_test)\n",
        "print(\"\\nAccuracy on budget dataset: \", accuracy)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:66: DeprecationWarning: Class Imputer is deprecated; Imputer was deprecated in version 0.20 and will be removed in 0.22. Import impute.SimpleImputer from sklearn instead.\n",
            "  warnings.warn(msg, category=DeprecationWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Accuracy on budget dataset:  0.3539941290362876\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1WuUzfsjQd7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "0ae9b3eb-4f52-4335-aa73-14f8631f6528"
      },
      "source": [
        "# Import random forest classifer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Edit model step in pipeline\n",
        "pl = Pipeline([\n",
        "        ('union', FeatureUnion(\n",
        "            transformer_list = [\n",
        "                ('numeric_features', Pipeline([\n",
        "                    ('selector', get_numeric_data),\n",
        "                    ('imputer', Imputer())\n",
        "                ])),\n",
        "                ('text_features', Pipeline([\n",
        "                    ('selector', get_text_data),\n",
        "                    ('vectorizer', CountVectorizer())\n",
        "                ]))\n",
        "             ]\n",
        "        )),\n",
        "        ('clf', RandomForestClassifier())\n",
        "    ])\n",
        "\n",
        "# Fit to the training data\n",
        "pl.fit(X_train, y_train)\n",
        "\n",
        "# Compute and print accuracy\n",
        "accuracy = pl.score(X_test, y_test)\n",
        "print(\"\\nAccuracy on budget dataset: \", accuracy)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:66: DeprecationWarning: Class Imputer is deprecated; Imputer was deprecated in version 0.20 and will be removed in 0.22. Import impute.SimpleImputer from sklearn instead.\n",
            "  warnings.warn(msg, category=DeprecationWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Accuracy on budget dataset:  0.9045031540815689\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNI6Ll9ZjwdS",
        "colab_type": "text"
      },
      "source": [
        "Can you make it better? Try changing the parameter n_estimators of RandomForestClassifier(), whose default value is 10, to 15."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhgkjykCjrpg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "77243ca9-f6cf-416a-caef-e6c7c1eff5af"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Add model step to pipeline: pl\n",
        "pl = Pipeline([\n",
        "        ('union', FeatureUnion(\n",
        "            transformer_list = [\n",
        "                ('numeric_features', Pipeline([\n",
        "                    ('selector', get_numeric_data),\n",
        "                    ('imputer', Imputer())\n",
        "                ])),\n",
        "                ('text_features', Pipeline([\n",
        "                    ('selector', get_text_data),\n",
        "                    ('vectorizer', CountVectorizer())\n",
        "                ]))\n",
        "             ]\n",
        "        )),\n",
        "        ('clf', RandomForestClassifier(n_estimators=15))\n",
        "    ])\n",
        "\n",
        "# Fit to the training data\n",
        "pl.fit(X_train, y_train)\n",
        "\n",
        "# Compute and print accuracy\n",
        "accuracy = pl.score(X_test, y_test)\n",
        "print(\"\\nAccuracy on budget dataset: \", accuracy)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:66: DeprecationWarning: Class Imputer is deprecated; Imputer was deprecated in version 0.20 and will be removed in 0.22. Import impute.SimpleImputer from sklearn instead.\n",
            "  warnings.warn(msg, category=DeprecationWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Accuracy on budget dataset:  0.9121104240834427\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_kviw7biElQ",
        "colab_type": "text"
      },
      "source": [
        "### Winner Model\n",
        "(1)\n",
        "- tokenize on punctuation\n",
        "- unigram + bigram\n",
        "\n",
        "(2)\n",
        "- interaction term: mathmatically describe when tokens appear together. eg. x3 = x1*x2\n",
        "- adding interaction terms also increases the number of features dramatically\n",
        "- polynimialfeatures(degree=True,interaction_only=True,include_bias=False).fit_transform(x)\n",
        "- SparseInteractions(degree=True).fit_transform(x).toarray() : takes in sparse matric(which is output by count vectorizer)\n",
        "(3)\n",
        "- we have interaction terms and ngrams, thus need better efficiency\n",
        "- we can use hashing (increases memory efficiency)\n",
        "- limits the putput size\n",
        "- the original paper says: even if multiple tokens map to the same hash, the model accuracy doesnt change much"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXpv8g1ancap",
        "colab_type": "text"
      },
      "source": [
        "By explicitly stating how many possible outputs the hashing function may have, we limit the size of the objects that need to be processed. With these limits known, computation can be made more efficient and we can get results faster, even on large datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I929viz0jER1",
        "colab_type": "text"
      },
      "source": [
        "**Special functions:** You'll notice a couple of new steps provided in the pipeline in this and many of the remaining exercises. Specifically, the dim_red step following the vectorizer step , and the scale step preceeding the clf (classification) step.\n",
        "\n",
        "These have been added in order to account for the fact that you're using a reduced-size sample of the full dataset in this course. To make sure the models perform as the expert competition winner intended, we have to apply a dimensionality reduction technique, which is what the dim_red step does, and we have to scale the features to lie between -1 and 1, which is what the scale step does.\n",
        "\n",
        "The dim_red step uses a scikit-learn function called SelectKBest(), applying something called the chi-squared test to select the K \"best\" features. The scale step uses a scikit-learn function called MaxAbsScaler() in order to squash the relevant features into the interval -1 to 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6Ew8CtMhWmD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "9a3f3f9b-de63-4c88-e489-e54cf151c78e"
      },
      "source": [
        "# Import pipeline\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Import classifiers\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "\n",
        "# Import CountVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Import other preprocessing modules\n",
        "from sklearn.preprocessing import Imputer\n",
        "from sklearn.feature_selection import chi2, SelectKBest\n",
        "\n",
        "# Select 300 best features\n",
        "chi_k = 300\n",
        "\n",
        "# Import functional utilities\n",
        "from sklearn.preprocessing import FunctionTransformer, MaxAbsScaler\n",
        "from sklearn.pipeline import FeatureUnion\n",
        "\n",
        "# Perform preprocessing\n",
        "get_text_data = FunctionTransformer(combine_text_columns, validate=False)\n",
        "get_numeric_data = FunctionTransformer(lambda x: x[NUMERIC_COLUMNS], validate=False)\n",
        "\n",
        "# Create the token pattern: TOKENS_ALPHANUMERIC\n",
        "TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)'\n",
        "\n",
        "# Instantiate pipeline: pl\n",
        "pl = Pipeline([\n",
        "        ('union', FeatureUnion(\n",
        "            transformer_list = [\n",
        "                ('numeric_features', Pipeline([\n",
        "                    ('selector', get_numeric_data),\n",
        "                    ('imputer', Imputer())\n",
        "                ])),\n",
        "                ('text_features', Pipeline([\n",
        "                    ('selector', get_text_data),\n",
        "                    ('vectorizer', CountVectorizer(token_pattern=TOKENS_ALPHANUMERIC,\n",
        "                                                   ngram_range=(1,2))),\n",
        "                    ('dim_red', SelectKBest(chi2, chi_k))\n",
        "                ]))\n",
        "             ]\n",
        "        )),\n",
        "        ('scale', MaxAbsScaler()),\n",
        "        ('clf', OneVsRestClassifier(LogisticRegression()))\n",
        "    ])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:66: DeprecationWarning: Class Imputer is deprecated; Imputer was deprecated in version 0.20 and will be removed in 0.22. Import impute.SimpleImputer from sklearn instead.\n",
            "  warnings.warn(msg, category=DeprecationWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmZGTSz_mGq6",
        "colab_type": "text"
      },
      "source": [
        "https://github.com/drivendataorg/box-plots-sklearn/blob/master/src/features/SparseInteractions.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z49egottjxHJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from itertools import combinations\n",
        "\n",
        "import numpy as np\n",
        "from scipy import sparse\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "\n",
        "class SparseInteractions(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, degree=2, feature_name_separator=\"_\"):\n",
        "        self.degree = degree\n",
        "        self.feature_name_separator = feature_name_separator\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        if not sparse.isspmatrix_csc(X):\n",
        "            X = sparse.csc_matrix(X)\n",
        "\n",
        "        if hasattr(X, \"columns\"):\n",
        "            self.orig_col_names = X.columns\n",
        "        else:\n",
        "            self.orig_col_names = np.array([str(i) for i in range(X.shape[1])])\n",
        "\n",
        "        spi = self._create_sparse_interactions(X)\n",
        "        return spi\n",
        "\n",
        "    def get_feature_names(self):\n",
        "        return self.feature_names\n",
        "\n",
        "    def _create_sparse_interactions(self, X):\n",
        "        out_mat = []\n",
        "        self.feature_names = self.orig_col_names.tolist()\n",
        "\n",
        "        for sub_degree in range(2, self.degree + 1):\n",
        "            for col_ixs in combinations(range(X.shape[1]), sub_degree):\n",
        "                # add name for new column\n",
        "                name = self.feature_name_separator.join(self.orig_col_names[list(col_ixs)])\n",
        "                self.feature_names.append(name)\n",
        "\n",
        "                # get column multiplications value\n",
        "                out = X[:, col_ixs[0]]\n",
        "                for j in col_ixs[1:]:\n",
        "                    out = out.multiply(X[:, j])\n",
        "\n",
        "                out_mat.append(out)\n",
        "\n",
        "        return sparse.hstack([X] + out_mat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3kzd2hwcmHzI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "4e0da616-a787-4e94-99aa-ce58ba611569"
      },
      "source": [
        "# Instantiate pipeline: pl\n",
        "pl = Pipeline([\n",
        "        ('union', FeatureUnion(\n",
        "            transformer_list = [\n",
        "                ('numeric_features', Pipeline([\n",
        "                    ('selector', get_numeric_data),\n",
        "                    ('imputer', Imputer())\n",
        "                ])),\n",
        "                ('text_features', Pipeline([\n",
        "                    ('selector', get_text_data),\n",
        "                    ('vectorizer', CountVectorizer(token_pattern=TOKENS_ALPHANUMERIC,\n",
        "                                                   ngram_range=(1, 2))),  \n",
        "                    ('dim_red', SelectKBest(chi2, chi_k))\n",
        "                ]))\n",
        "             ]\n",
        "        )),\n",
        "        ('int', SparseInteractions(degree=2)),\n",
        "        ('scale', MaxAbsScaler()),\n",
        "        ('clf', OneVsRestClassifier(LogisticRegression()))\n",
        "    ])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:66: DeprecationWarning: Class Imputer is deprecated; Imputer was deprecated in version 0.20 and will be removed in 0.22. Import impute.SimpleImputer from sklearn instead.\n",
            "  warnings.warn(msg, category=DeprecationWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qtM0m_2XmX-T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "2eb3c78e-b4a0-45e3-895c-b3770b73f0aa"
      },
      "source": [
        "# Import HashingVectorizer\n",
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "\n",
        "# Get text data: text_data\n",
        "text_data = combine_text_columns(X_train)\n",
        "\n",
        "# Create the token pattern: TOKENS_ALPHANUMERIC\n",
        "TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)' \n",
        "\n",
        "# Instantiate the HashingVectorizer: hashing_vec\n",
        "hashing_vec = HashingVectorizer(token_pattern=TOKENS_ALPHANUMERIC)\n",
        "\n",
        "# Fit and transform the Hashing Vectorizer\n",
        "hashed_text = hashing_vec.fit_transform(text_data)\n",
        "\n",
        "# Create DataFrame and print the head\n",
        "hashed_df = pd.DataFrame(hashed_text.data)\n",
        "print(hashed_df.head())"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "          0\n",
            "0  0.377964\n",
            "1  0.755929\n",
            "2  0.377964\n",
            "3  0.377964\n",
            "4  0.235702\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xn5l1ECmn37Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "73171c2c-f9c9-49d3-97c4-4c41f1fc0901"
      },
      "source": [
        "# Import the hashing vectorizer\n",
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "\n",
        "# Instantiate the winning model pipeline: pl\n",
        "pl = Pipeline([\n",
        "        ('union', FeatureUnion(\n",
        "            transformer_list = [\n",
        "                ('numeric_features', Pipeline([\n",
        "                    ('selector', get_numeric_data),\n",
        "                    ('imputer', Imputer())\n",
        "                ])),\n",
        "                ('text_features', Pipeline([\n",
        "                    ('selector', get_text_data),\n",
        "                    ('vectorizer', HashingVectorizer(token_pattern=TOKENS_ALPHANUMERIC,\n",
        "                                                     norm=None, binary=False,\n",
        "                                                     ngram_range=(1,2))),\n",
        "                    ('dim_red', SelectKBest(chi2, chi_k))\n",
        "                ]))\n",
        "             ]\n",
        "        )),\n",
        "        ('int', SparseInteractions(degree=2)),\n",
        "        ('scale', MaxAbsScaler()),\n",
        "        ('clf', OneVsRestClassifier(LogisticRegression()))\n",
        "    ])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:66: DeprecationWarning: Class Imputer is deprecated; Imputer was deprecated in version 0.20 and will be removed in 0.22. Import impute.SimpleImputer from sklearn instead.\n",
            "  warnings.warn(msg, category=DeprecationWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOuDWchzpkLE",
        "colab_type": "text"
      },
      "source": [
        "analyzer : string, {â€˜wordâ€™, â€˜charâ€™, â€˜char_wbâ€™} or callable\n",
        "Whether the feature should be made of word or character n-grams."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xo6mwuivokSa",
        "colab_type": "text"
      },
      "source": [
        "https://github.com/datacamp/course-resources-ml-with-experts-budgets/blob/master/notebooks/1.0-full-model.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Vgoe4Loouqm",
        "colab_type": "text"
      },
      "source": [
        "**Further Work:**\n",
        "- stem/lemmatize\n",
        "- models: svm,random forest, naive bayes, knn, ensemble\n",
        "- handling nans differently than the default imputer\n",
        "- optimization: gridsearch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zV-6b3MJoSLu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}